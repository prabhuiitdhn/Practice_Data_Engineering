"""
https://scikit-learn.org/stable/auto_examples/preprocessing/plot_target_encoder.html#sphx-glr-auto-examples-preprocessing-plot-target-encoder-py

Data has two types: Numerical and categorical
Numerical: Made of numbers(weight, age number of children and etc,)
        - continuous: Age, weight, blood level
        - discrete: number of children, shoe size

categorical: Made of words [Eye color, Blood type, gender]
            - Ordinal(ordered: one-after-one): Data has hierarchy(pain severity, Satisfaction, rating, mood)
            - Nominal (Unordered: Data has no hierarchy): Eye color, dog breed, blood type

Machine learning algorithms typically require numerical inputs, so categorical variables need to be transformed in a way
that the model can understand. Here are some common techniques to handle categorical variables in machine learning:

1. Label encoding:
    - for the ordinal value, An integer can be assigned on their order.

2. One hot encoding:
    - for nominal variable, one-hot encoding creates a binary column for each category.
    Each column represents the presence or absence of that category.
    This method prevents the model from inferring non-existent ordinal relationships.
    It increases the dimensionality of the data but can improve model accuracy.

3. Binary encoding:
    - It represents each category with binary digits, reducing the dimensionality compared to one-hot encoding.
    - Particularly useful when dealing with high-cardinality categorical variables.

4. Frequency Encoding:
    - Replaces each category with its frequency in the dataset.
    - Can help capture information about the distribution of categories.

5. Target Encoding [Mean encoding]
    - Replaces each category with the mean of the target variable for that category
    - Useful when the target variable exhibits a strong relationship with the categorical variable.
    - Prone to over fitting, so regularization techniques are often applied.

6. Embedding layers [for neural networks]
    - embedding layers can be used to learn representations for categorical variables.
    - embeddings capture relationships between categories and can improve model performance.

7. Hashing Trick:
   - Converts categorical values into a fixed number of features using hash functions.
   - Useful for reducing the dimensionality of high-cardinality categorical variables.

8. Leave-one-out-Encoding
    - Similar to target encoding, but uses all data except the current instance to calculate the mean target.
    - Helps mitigate the risk of overfitting in target encoding.

9. Ordinal Encoding:
    - Assigns integer values based on the order of the categories.
"""

"""The TargetEncoder uses the value of the target to encode each categorical feature. 
In this example, we will compare three different approaches for handling categorical features: TargetEncoder, OrdinalEncoder, 
OneHotEncoder and dropping the category. """

import pandas as pd

pd.set_option('display.max_columns', None)  # THIS SHOWS THE MAXIMUM COLUMN OF DATA USING PANDAS

import matplotlib.pyplot as plt

from sklearn.datasets import fetch_openml

wine_reviews = fetch_openml(data_id=42074, as_frame=True, parser="pandas")
df = wine_reviews.frame
# print(df.head())

"""

country	     description  designation	points	price	province	region_1	region_2	variety 	winery
0	US	     This tremendous 100% varietal wine hails from ...	Martha's Vineyard	96	235.0	California	Napa Valley	Napa	Cabernet Sauvignon	Heitz
1	Spain	 Ripe aromas of fig, blackberry and cassis are ...	Carodorum Selección Especial Reserva	96	110.0	Northern Spain	Toro	NaN	Tinta de Toro	Bodega Carmen Rodríguez
2	US	     Mac Watson honors the memory of a wine once ma...	Special Selected Late Harvest	96	90.0	California	Knights Valley	Sonoma	Sauvignon Blanc	Macauley
3	US	     This spent 20 months in 30% new French oak, an...	Reserve	96	65.0	Oregon	Willamette Valley	Willamette Valley	Pinot Noir	Ponzi
4	France	 This is the top wine from La Bégude, named aft...	La Brûlade	95	66.0	Provence	Bandol	NaN	Provence red blend	Domaine de la Bégude
"""

numerical_features = ["price"]
categorical_features = [
    "country",
    "province",
    "region_1",
    "region_2",
    "variety",
    "winery",
]
target_name = "points"  # using the the value "points" feature encodes each categorical feature.
X = df[numerical_features + categorical_features]
y = df[target_name]
_ = y.hist()
# plt.show()


# Training and Evaluating Pipelines with Different Encoders

from sklearn.compose import ColumnTransformer # Applies transformers to columns of an array or pandas DataFrame.This
                                            # estimator allows different columns or column subsets of the input to be transformed separately and the features
                                            # generated by each transformer will be concatenated to form a single feature space.

from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, TargetEncoder

categorical_preprocessors = [
    ("drop", "drop"),
    ("Ordinal", OrdinalEncoder(handle_unknown="use_encoded_value", unknown_value=-1)),
    ("One_hot", OneHotEncoder(handle_unknown="ignore", max_categories=20, sparse_output=False)),
    ("target", TargetEncoder(target_type="continuous"))
]

# Next, we evaluate the models using cross validation and record the results:

from sklearn.ensemble import HistGradientBoostingRegressor
from sklearn.model_selection import cross_validate
from sklearn.pipeline import make_pipeline # Class for creating a pipeline of transforms with a final estimator

n_cv_folds = 3
max_iterations = 20
results = []


def evaluate_model_and_store(name, pipe):
    # Evaluate metric(s) by cross-validation and also record fit/score times.
    result = cross_validate(
        pipe, X, y, scoring="neg_root_mean_squared_error",
        cv=n_cv_folds,
        return_train_score=True
    )

    rmse_test_score = -result["test_score"]
    rmse_train_score = -result["train_score"]

    results.append(
        {
            "preprocessor": name,
            "rmse_test_mean": rmse_test_score.mean(),
            "rmse_test_std": rmse_train_score.std(),
            "rmse_train_mean": rmse_train_score.mean(),
            "rmse_train_std": rmse_train_score.std(),
        }
    )


for name, categorical_preprocessor in categorical_preprocessors:
    preprocessor = ColumnTransformer(
        [
            ("numerical", "passthrough", numerical_features),
            ("categorical", categorical_preprocessor, categorical_features),
        ]
    )

    pipe = make_pipeline(
        preprocessor, HistGradientBoostingRegressor(random_state=0, max_iter=max_iterations)
    )

    evaluate_model_and_store(name, pipe)


n_unique_categories = df[categorical_features].nunique().sort_values(ascending=False)
print("Unique features:", n_unique_categories)

high_cardinality_features = n_unique_categories[n_unique_categories > 255].index
print("High_cardinality_features", high_cardinality_features)
low_cardinality_features = n_unique_categories[n_unique_categories <= 255].index
print("low_cardinality_features", low_cardinality_features)

mixed_encoded_preprocessor = ColumnTransformer(
    [
        ("numerical", "passthrough", numerical_features),
        (
            "high_cardinality",
            TargetEncoder(target_type="continuous"),
            high_cardinality_features,
        ),
        (
            "low_cardinality",
            OrdinalEncoder(handle_unknown="use_encoded_value", unknown_value=-1),
            low_cardinality_features,
        ),
    ],
    verbose_feature_names_out=False,
)


# The output of the of the preprocessor must be set to pandas so the
# gradient boosting model can detect the low cardinality features.
mixed_encoded_preprocessor.set_output(transform="pandas")
mixed_pipe = make_pipeline(
    mixed_encoded_preprocessor,
    HistGradientBoostingRegressor(
        random_state=0,
        max_iter=max_iterations,
        categorical_features=low_cardinality_features
    ),
)
print("mixed Pipeline:", mixed_pipe)

evaluate_model_and_store("mixed_target", mixed_pipe)


import matplotlib.pyplot as plt
import pandas as pd

results_df = (
    pd.DataFrame(results).set_index("preprocessor").sort_values("rmse_test_mean")
)

fig, (ax1, ax2) = plt.subplots(
    1, 2, figsize=(12, 8), sharey=True, constrained_layout=True
)
xticks = range(len(results_df))
name_to_color = dict(
    zip((r["preprocessor"] for r in results), ["C0", "C1", "C2", "C3", "C4"])
)

for subset, ax in zip(["test", "train"], [ax1, ax2]):
    mean, std = f"rmse_{subset}_mean", f"rmse_{subset}_std"
    data = results_df[[mean, std]].sort_values(mean)
    ax.bar(
        x=xticks,
        height=data[mean],
        yerr=data[std],
        width=0.9,
        color=[name_to_color[name] for name in data.index],
    )
    ax.set(
        title=f"RMSE ({subset.title()})",
        xlabel="Encoding Scheme",
        xticks=xticks,
        xticklabels=data.index,
    )
    plt.show()